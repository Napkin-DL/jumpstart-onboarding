{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart 모델 온보딩 (Deep Java Library)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML 모델 패키징 프로세스\n",
    "\n",
    "<img src=\"images/ml-model-publishing-workflow.png\"/>\n",
    "\n",
    "다음 다이어그램은 ML 모델 패키징 프로세스의 개요를 제공합니다.\n",
    "\n",
    "\n",
    "- **1단계** 모델 아티팩트 및 서빙/스코어링 로직 저장\n",
    "- **2단계** 추론을 수행하는 SageMaker에서 모델을 호스팅하는 데 사용되는 컨테이너를 생성하고 ECR에 푸시\n",
    "- **3단계** SageMaker에서 모델을 성공적으로 호스팅할 수 있는 컨테이너 검증\n",
    "- **4단계** ML 모델을 모델 패키지로 패키징\n",
    "- **5단계** Amazon SageMaker에 배포하여 ML 모델 패키지 검증\n",
    "- **6단계** AWS Marketplace에 ML 모델 등록\n",
    "\n",
    "> **참고**: 모든 로컬 작업은 최적의 성능과 호환성을 위해 반드시 GPU가 지원되는 SageMaker Notebook 인스턴스에서 수행되어야 합니다.\n",
    "> \n",
    "> [Deploy models with DJL Serving](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-models-frameworks-djl-serving.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip --quiet\n",
    "    !{sys.executable} -m pip install -U sagemaker --quiet\n",
    "    !{sys.executable} -m pip install -U transformers --quiet\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_name_or_path='Qwen/Qwen2.5-7B-Instruct'\n",
    "cache_dir = f'{Path.cwd()}/cache_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jinja2\n",
    "jinja_env = jinja2.Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 1**] 모델 아티팩트 및 서빙/스코어링 로직 저장\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_model(hf_model_id, local_model_path, ignore_patterns=None):\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    # 기본적으로 무시할 패턴 설정 (safetensors는 반드시 포함해야 함)\n",
    "    if ignore_patterns is None:\n",
    "        ignore_patterns = [\"*.ckpt\"]  # 큰 체크포인트 파일만 무시\n",
    "    \n",
    "    local_model_path = Path(local_model_path)\n",
    "    print(f\"Downloading model to: {local_model_path}\")\n",
    "    \n",
    "    # 디렉토리 준비\n",
    "    if os.path.exists(local_model_path):\n",
    "        print(f\"Model directory already exists. Updating files at: {local_model_path}\")\n",
    "    else:\n",
    "        os.makedirs(local_model_path, exist_ok=True)\n",
    "        print(f\"Created new model directory: {local_model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # 모델 다운로드\n",
    "        allow_patterns = [\"*.json\", \"*.safetensors\", \"*.pt\", \"*.txt\", \"*.model\", \"*.tiktoken\", \"*.gguf\"]\n",
    "        \n",
    "        snapshot_download(\n",
    "            repo_id=hf_model_id,\n",
    "            local_dir=local_model_path,\n",
    "            local_dir_use_symlinks=False,\n",
    "            ignore_patterns=ignore_patterns,\n",
    "            allow_patterns=allow_patterns,\n",
    "        )\n",
    "        \n",
    "        # 다운로드 검증\n",
    "        model_files = os.listdir(local_model_path)\n",
    "        print(f\"Downloaded files: {model_files}\")\n",
    "        \n",
    "        # 필수 파일 확인\n",
    "        if not os.path.exists(os.path.join(local_model_path, \"config.json\")):\n",
    "            raise FileNotFoundError(\"config.json이 다운로드되지 않았습니다!\")\n",
    "        \n",
    "        # 가중치 파일 확인\n",
    "        weight_found = False\n",
    "        for file in model_files:\n",
    "            if file.endswith('.bin') or file.endswith('.safetensors'):\n",
    "                weight_found = True\n",
    "                break\n",
    "                \n",
    "        if not weight_found:\n",
    "            raise FileNotFoundError(\"모델 가중치 파일이 다운로드되지 않았습니다!\")\n",
    "            \n",
    "        print(f\"모델 다운로드 완료: {hf_model_id}\")\n",
    "        return local_model_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"모델 다운로드 오류: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf serve/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_model(model_name_or_path, f\"serve/checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serve/serving.properties\n",
    "option.model_id=/opt/ml/model/checkpoint\n",
    "option.dtype=fp16\n",
    "option.trust_remote_code=True\n",
    "option.tensor_parallel_degree=max\n",
    "option.gpu_memory_utilization=.95\n",
    "option.max_model_len=10000\n",
    "option.paged_attention=False\n",
    "option.enable_streaming=False\n",
    "option.kv_cache_dtype=fp8_e4m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 2**] 추론 수행을 위한 SageMaker 컨테이너 생성 및 ECR 등록\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu126\"\n",
    "docker_account_id = image_uri.split('.')[0]\n",
    "print(f'image_uri: {image_uri} \\ndocker_account_id : {docker_account_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf docker && mkdir docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu126\n",
    "\n",
    "ENV TZ=Asia/Seoul\n",
    "RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV LANG C.UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf shell && mkdir shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile shell/build_and_push.sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name={{algorithm_name}}\n",
    "image_tag={{image_tag}}\n",
    "\n",
    "cd {{root_dir}}/docker\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:${image_tag}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin \"{{docker_account_id}}.dkr.ecr.${region}.amazonaws.com\"\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -f Dockerfile -t ${fullname} .\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algorithm_name='js-on-boarding-test'\n",
    "image_tag='qwen-2-5-7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = jinja_env.from_string(Path(\"shell/build_and_push.sh\").open().read())\n",
    "Path(\"shell/build_and_push.sh\").open(\"w\").write(template.render(algorithm_name=algorithm_name, image_tag=image_tag, root_dir=os.getcwd(), docker_account_id=docker_account_id))\n",
    "# !pygmentize shell/build_and_push.sh | cat -n\n",
    "!chmod +x shell/build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!./shell/build_and_push.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account = sagemaker.Session().account_id()\n",
    "region = sagemaker.Session().boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecr_image_uri=f\"{account}.dkr.ecr.{region}.amazonaws.com/{algorithm_name}:{image_tag}\"\n",
    "ecr_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 3**] SageMaker에서 모델을 성공적으로 호스팅할 수 있는 컨테이너 검증\n",
    "---\n",
    "\n",
    "SageMaker 호스팅 엔드포인트로 배포하기 전에 로컬 모드 엔드포인트로 배포할 수 있습니다. 로컬 모드는 현재 개발 중인 환경에서 도커 컨테이너를 실행하여 SageMaker 프로세싱/훈련/추론 작업을 에뮬레이트할 수 있습니다. 추론 작업의 경우는 Amazon ECR의 딥러닝 프레임워크 기반 추론 컨테이너를 로컬로 가져오고(docker pull) 컨테이너를 실행하여(docker run) 모델 서버를 시작합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Model Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket=sagemaker_session.default_bucket()\n",
    "prefix='qwen-2-5-7B-instruct/model_data_tests'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rm -rf serve/.ipynb_checkpoints/\n",
    "!sudo rm -rf serve/__pycache__/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('shell', exist_ok=True)\n",
    "os.makedirs('model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile shell/pretrained_model_compression_upload.sh\n",
    "\n",
    "cd serve\n",
    "tar cvf - * | pigz > ../model.tar.gz\n",
    "\n",
    "cd ..\n",
    "mv model.tar.gz ./model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!sh ./shell/pretrained_model_compression_upload.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker Endpoint (Local Mode)\n",
    "\n",
    "로컬 모드는 필수로 수행할 필요는 없지만, 디버깅에 많은 도움이 됩니다. 또한, 로컬 모드 사용 시에는 모델을 S3에 반드시 업로드할 필요 없이 로컬 디렉터리에서도 로드할 수 있습니다. (`container` 변수 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = True\n",
    "# local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    from sagemaker.local import LocalSession\n",
    "    instance_type = \"local_gpu\"\n",
    "    sm_session = LocalSession()\n",
    "    sm_client = sagemaker.local.LocalSagemakerClient()\n",
    "    smr_client = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "    model_data=f\"file://{Path.cwd()}/model/model.tar.gz\"\n",
    "else:\n",
    "    instance_type = \"ml.g5.2xlarge\"\n",
    "    sm_session = sagemaker.Session()\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    model_data = model_data_url\n",
    "\n",
    "instance_count = 1\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = f\"qwen-model-{ts}\"\n",
    "endpoint_config_name = f\"qwen-endpoint-config-{ts}\"\n",
    "endpoint_name = f\"qwen-endpoint-{ts}\"\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = {}\n",
    "container = {\n",
    "    \"Image\": ecr_image_uri,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "    \"Environment\": env\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, \n",
    "    ExecutionRoleArn=role, \n",
    "    PrimaryContainer=container,\n",
    ")\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "#print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker kill 695ddd1fefd5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "\n",
    "# 요청 데이터 준비\n",
    "request_data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    \"max_new_tokens\": 256  # 더 작은 값으로 제한\n",
    "}\n",
    "\n",
    "\n",
    "# API 호출\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Accept=\"application/json\",\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(request_data).encode('utf-8')\n",
    ")\n",
    "\n",
    "\n",
    "# 응답 처리\n",
    "response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "print(response_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    EndpointConfigName = response['EndpointConfigName']\n",
    "    \n",
    "    response = client.describe_endpoint_config(EndpointConfigName=EndpointConfigName)\n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "    \n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint_config(EndpointConfigName=EndpointConfigName) \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "   \n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint_config: {EndpointConfigName}')     \n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, endpoint_name)\n",
    "!sudo rm -rf /tmp/tmp*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Endpoint 에서 확인 (원격에서 호스팅)\n",
    "> **⚠️ 주의** : 테스트 후 **반드시** 인스턴스 삭제가 필요합니다. 콘솔에서 SageMakerAI - Inference 에 SageMaker Endpoint가 삭제되었는지 확인해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model을 S3로 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_name = \"./model/model.tar.gz\"\n",
    "model_data_url = sagemaker_session.upload_data(path=tar_name, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "# Set to True to enable SageMaker to run locally\n",
    "# local_mode = True\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    from sagemaker.local import LocalSession\n",
    "    instance_type = \"local_gpu\"\n",
    "    sm_session = LocalSession()\n",
    "    sm_client = sagemaker.local.LocalSagemakerClient()\n",
    "    smr_client = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "    model_data=f\"file://{Path.cwd()}/model/model.tar.gz\"\n",
    "else:\n",
    "    instance_type = \"ml.g5.2xlarge\"\n",
    "    sm_session = sagemaker.Session()\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    model_data = model_data_url\n",
    "\n",
    "instance_count = 1\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = f\"qwen-model-{ts}\"\n",
    "endpoint_config_name = f\"qwen-endpoint-config-{ts}\"\n",
    "endpoint_name = f\"qwen-endpoint-{ts}\"\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_endpoint(endpoint_name, request_data):\n",
    "    import json\n",
    "    \n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Accept=\"application/json\",\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(request_data).encode('utf-8')\n",
    "    )\n",
    "    data = response[\"Body\"].read()\n",
    "    output = json.loads(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = {\n",
    "}\n",
    "\n",
    "container = {\n",
    "    \"Image\": ecr_image_uri,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "    \"Environment\": env\n",
    "}\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "def make_console_link(region, endpoint_name, task='[SageMaker LLM Serving]'):\n",
    "    endpoint_link = f'<b> {task} <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">Check Endpoint Status</a></b>'   \n",
    "    return endpoint_link\n",
    "\n",
    "endpoint_link = make_console_link(region, endpoint_name)\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(30)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_endpoint(endpoint_name, prompt):\n",
    "    import json\n",
    "    # 요청 데이터 준비\n",
    "    request_data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_new_tokens\": 256  # 더 작은 값으로 제한\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # API 호출\n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Accept=\"application/json\",\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(request_data).encode('utf-8')\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 응답 처리\n",
    "    response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    print(response_body)\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = invoke_endpoint(endpoint_name, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    EndpointConfigName = response['EndpointConfigName']\n",
    "    \n",
    "    response = client.describe_endpoint_config(EndpointConfigName=EndpointConfigName)\n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "    \n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint_config(EndpointConfigName=EndpointConfigName) \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "   \n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint_config: {EndpointConfigName}')     \n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 4**] ML 모델을 모델 패키지로 패키징\n",
    "---\n",
    "이 **step**에서는 아티팩트(ECR 이미지 및 학습된 모델 아티팩트)를 ModelPackage로 패키징하는 방법을 살펴봅니다. 이 작업을 완료하면 AWS 마켓플레이스에서 제품을 사전 학습된 모델로 등록할 수 있습니다.\n",
    "\n",
    "**Note:** 모델을 여러 하드웨어 유형(CPU/GPU/Inferentia)에 배포할 수 있는 경우, 일반적으로 사용되는 컨테이너 이미지가 각각 다르기 때문에 각각에 대해 모델패키지를 생성하고 MP 목록에 다른 버전으로 추가해야 합니다.  \n",
    "\n",
    "### 모델 패키지 사전 준비\n",
    "모델 패키지는 추론에 필요한 모든 요소를 패키지로 묶은 모델 아티팩트에 대한 재사용 가능한 추상화 형태입니다. 이는 모델 데이터 위치(선택 사항)와 함께 사용할 추론 이미지를 정의하는 추론 사양으로 구성됩니다. ModelPackage는 AWS 마켓플레이스에 판매자로 등록할 AWS 계정에서 생성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix='qwen-2-5-7B-instruct/model_data_tests'\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "account= boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_client = sagemaker_session.boto_session.client(\"s3\")\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data = f's3://{bucket}/{prefix}/model.tar.gz'\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algorithm_name='js-on-boarding-test'\n",
    "image_tag='qwen-2-5-7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecr_image_uri=f\"{account}.dkr.ecr.{region}.amazonaws.com/{algorithm_name}:{image_tag}\"\n",
    "ecr_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# <<YourSupportedContentTypes>>\n",
    "supported_content_types = [\"application/json\"] #[\"text/csv\", \"application/json\", \"application/json\", \"application/jsonlines\"]\n",
    "\n",
    "# <<YourSupportedResponseMIMETypes>>\n",
    "supported_response_MIME_types = [ \n",
    "    \"application/json\",\n",
    "    # \"text/csv\",\n",
    "    # \"application/jsonlines\",\n",
    "]\n",
    "\n",
    "supported_realtime_inference_instance_types = [\"ml.g5.2xlarge\"]\n",
    "supported_batch_transform_instance_types = [\"ml.g5.2xlarge\"] #  Don't use batch transform. And, the Batch Transform validation step is not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 패키지 생성\n",
    "모델 패키지 생성 프로세스에서는 다음을 지정해야 합니다:\n",
    "  1. 도커 이미지\n",
    "  2. 모델 아티팩트\n",
    "    - tar.gz 형태로 압축된 모델 아티팩트가 제공되어야 합니다.\n",
    "        \n",
    "판매자(및 구매자)에게 Amazon SageMaker에서 제품이 작동한다는 확신을 주기 위해, AWS Marketplace에 제품을 리스팅하기 전에 SageMaker는 기본적인 유효성 검사를 위와 같이 진행하였습니다. 이 유효성 검사 프로세스가 성공해야만 제품을 AWS Marketplace에 리스팅할 수 있습니다. 이 유효성 검사 프로세스는 사용자가 제공한 유효성 검사 프로필과 샘플 데이터를 사용하여 모델을 사용하여 계정에서 변환 작업을 생성하여 추론 이미지가 SageMaker에서 작동하는지 확인합니다.\n",
    "\n",
    "다음으로, ML 모델에 적합한 인스턴스 크기를 식별해야 하며, ML 모델 위에서 성능 테스트를 실행하여 이를 확인할 수 있습니다.\n",
    "\n",
    "**Note:** 모델 튜닝 외에도 인스턴스 유형을 식별할 때 모델의 요구 사항을 고려해야 합니다.  모델이 GPU 리소스를 사용하지 않는 경우 GPU 인스턴스 유형을 포함하지 마세요. 마찬가지로 모델이 GPU 리소스를 사용하지만 단일 GPU만 사용할 수 있는 경우, 여러 개의 GPU가 있는 인스턴스 유형을 포함하지 마세요. 성능상의 이점은 없이 사용자의 인프라 요금만 증가시킬 수 있기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트용 샘플이미지 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    \"max_new_tokens\": 256 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "s3_client = sagemaker_session.boto_session.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_line = json.dumps(payload)\n",
    "with open(\"input.jsonl\", \"w\") as f:\n",
    "    f.write(json_line)\n",
    "s3_client.put_object(Bucket=bucket, Key=\"validation-input-json/input.jsonl\", Body=json_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_file_name = \"input.jsonl\"\n",
    "validation_input_path = f\"s3://{bucket}/validation-input-json/\"\n",
    "validation_output_path = f\"s3://{bucket}/validation-output-jsonl/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "model_name = \"marketplace-model-qwen-test-1\" #\"<<YourModelName>>\"\n",
    "model_description = \"marketplace-model-qwen-test\" #\"<<YourModelDescription>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package = sagemaker_session.sagemaker_client.create_model_package(\n",
    "    ModelPackageName=model_name,\n",
    "    ModelPackageDescription=model_description,\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": ecr_image_uri,\n",
    "                \"ModelDataUrl\": model_data\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedTransformInstanceTypes\": supported_batch_transform_instance_types,\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": supported_realtime_inference_instance_types,\n",
    "        \"SupportedContentTypes\": supported_content_types,\n",
    "        \"SupportedResponseMIMETypes\": supported_response_MIME_types,\n",
    "    },\n",
    "    CertifyForMarketplace=True,  # Make sure to set this to True\n",
    "   ValidationSpecification={\n",
    "        'ValidationRole': role,\n",
    "        'ValidationProfiles': [\n",
    "            {\n",
    "                'ProfileName': \"validation\",\n",
    "                'TransformJobDefinition': {\n",
    "                    'MaxConcurrentTransforms': 1,\n",
    "                    'MaxPayloadInMB': 64,\n",
    "                    'BatchStrategy': 'SingleRecord',\n",
    "                    'TransformInput': {\n",
    "                        'DataSource': {\n",
    "                            'S3DataSource': {\n",
    "                                'S3DataType': 'S3Prefix',\n",
    "                                'S3Uri': f'{validation_input_path}input.jsonl'\n",
    "                            }\n",
    "                        },\n",
    "                        'ContentType': 'application/json',\n",
    "                        'CompressionType': 'None',\n",
    "                        'SplitType': 'None'\n",
    "                    },\n",
    "                    'TransformOutput': {\n",
    "                        'S3OutputPath': f'{validation_output_path}output.json',\n",
    "                        'Accept': 'application/json',\n",
    "                        'AssembleWith': 'None',\n",
    "                    },\n",
    "                    'TransformResources': {\n",
    "                        'InstanceType': 'ml.g5.2xlarge',\n",
    "                        'InstanceCount': 1,\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session.wait_for_model_package(model_package_name=model_name) # If failure occurs navigate to SageMaker Console > My marketplace model packages > select the failed ModelPackage for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음을 실행하기 전에, [Model Packages console from Amazon SageMaker](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources)을 열어서 모델 생성의 성공했는지를 확인해야 합니다.\n",
    "모델을 선택하고 **Validation** 탭을 열어서 validation 결과를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 5**] Amazon SageMaker에 배포하여 ML 모델 패키지 검증\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 패키지에서 모델 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package['ModelPackageArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage\n",
    "\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package[\"ModelPackageArn\"],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker 모델을 Endpoint로 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=supported_realtime_inference_instance_types[0],\n",
    "    endpoint_name=model_name,\n",
    ")\n",
    "model.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### boto3로 예시 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make use of your own example input data to test the Endpoint\n",
    "#input_json = '{\"text\": \"sample\"}'\n",
    "\n",
    "payload = {'input_image' : input_image.decode(\"utf-8\")}\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=model.endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "json.load(response[\"Body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS CLI로 예시 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "    --endpoint-name $model.endpoint_name \\\n",
    "    --body fileb://$validation_file_name \\\n",
    "    --content-type application/json \\\n",
    "    --region $sagemaker_session.boto_region_name \\\n",
    "    out.out\n",
    "    \n",
    "    \n",
    "# Print inference\n",
    "!head out.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 생성된 endpoint configuration 과 endpoint 정리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model.endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 모델은 필수가 아니므로 삭제해도 됩니다. \n",
    "- 배포 가능한 모델을 삭제한다는 점에 유의하세요. \n",
    "- 모델 패키지는 삭제하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AWS 마켓플레이스에 모델을 게시하려면 모델 패키지 ARN을 지정해야 합니다. 다음 모델 패키지 ARN을 복사합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package[\"ModelPackageArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 6**] Listing the ML model in AWS Marketplace\n",
    "---\n",
    "\n",
    "1.  모델 파트너는 AWS 마켓플레이스에서 [public profile](https://docs.aws.amazon.com/marketplace/latest/userguide/seller-registration-process.html#seller-public-profile)을 생성하고 seller로 등록합니다.\n",
    "마켓플레이스의 상품은 무료 상품으로 등록되므로 세금 정보를 제공할 필요가 없습니다.\n",
    "\n",
    "2. 세이지메이커 콘솔의 [Model Packages](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources) 섹션에서 이 노트북에서 생성한 엔티티를 찾을 수 있습니다. 성공적으로 생성되고 유효성이 검사되었다면 해당 엔티티를 선택하고 **Publish new ML Marketplace listing**를 선택할 수 있을 것입니다.\n",
    "\n",
    "<img src=\"images/publish-to-marketplace-action.png\"/>\n",
    "\n",
    "리스팅을 작성할 수 있는 [AWS Marketplace Management portal](https://aws.amazon.com/marketplace/management/ml-products/)로 리디렉션됩니다.\n",
    "\n",
    "<img src=\"images/listing.png\"/>\n",
    "\n",
    "1. 모델이 여러 하드웨어 유형을 대상으로 하는 경우 각 ModelPackage를 별도의 버전으로 목록에 추가하는 것을 잊지 마세요.\n",
    "2. 추가를 클릭하고 모델 정보를 입력합니다. Product visibility을 'Public'로 설정해야 합니다.\n",
    "\n",
    "<img src=\"images/public.png\"/>\n",
    "\n",
    "3. 테스트를 진행할 account 에 대해 모델 접근을 위한 Allowlist에 추가합니다. 예) account `171503325295`, `572320329544` and `559110549532` for access to the model. \n",
    "For region support select: `us-east-1, us-west-2, eu-west-1, eu-central-1, eu-west-2, ap-northeast-1, ap-south-1, ca-central-1, us-east-2, ap-northeast-2`\n",
    "<img src=\"images/allowlist-accs.png\"/>\n",
    "\n",
    "4. Pricing and terms 하에 pricing 모델을 설정합니다.\n",
    "**Inference based pricing (custom metering) at $0**\n",
    "\n",
    "(선택 사항) 컨테이너가 아래를 구현하지 않은 경우 이를 확인하고 다음을 진행하세요. \n",
    "\n",
    "```\n",
    "I confirm that my model package supports the response header for custom metering. Example response header: X-Amzn-Inference-Metering:\n",
    "{\"Dimension\": \"inference.count\", \"ConsumedUnits\": 3}\n",
    "I understand that in absence of this header, default metering will be used instead.\n",
    "```\n",
    "\n",
    "<img src=\"images/inference-based-pricing.png\"/>\n",
    "\n",
    "5. Listing 상태는 다음과 같이 표시되어야 합니다:\n",
    "**Do not click Sign off and publish**\n",
    "\n",
    "<img src=\"images/status-1.png\"/>\n",
    "\n",
    "6. Vissibility status of the listing should be `Limited`.\n",
    "\n",
    "<img src=\"images/status-2.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "* [Publishing your product in AWS Marketplace](https://docs.aws.amazon.com/marketplace/latest/userguide/ml-publishing-your-product-in-aws-marketplace.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
