{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart 모델 온보딩 (TGI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML 모델 패키징 프로세스\n",
    "\n",
    "<img src=\"images/ml-model-publishing-workflow.png\"/>\n",
    "\n",
    "다음 다이어그램은 ML 모델 패키징 프로세스의 개요를 제공합니다.\n",
    "\n",
    "\n",
    "- **1단계** 모델 아티팩트 및 서빙/스코어링 로직 저장\n",
    "- **2단계** 추론을 수행하는 SageMaker에서 모델을 호스팅하는 데 사용되는 컨테이너를 생성하고 ECR에 푸시\n",
    "- **3단계** SageMaker에서 모델을 성공적으로 호스팅할 수 있는 컨테이너 검증\n",
    "- **4단계** ML 모델을 모델 패키지로 패키징\n",
    "- **5단계** Amazon SageMaker에 배포하여 ML 모델 패키지 검증\n",
    "- **6단계** AWS Marketplace에 ML 모델 등록\n",
    "\n",
    "> **참고**: 모든 로컬 작업은 최적의 성능과 호환성을 위해 반드시 GPU가 지원되는 SageMaker Notebook 인스턴스에서 수행되어야 합니다.\n",
    "> \n",
    "> [Deploy models with DJL Serving](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-models-frameworks-djl-serving.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi\n",
    "\n",
    "sudo curl -L \"https://github.com/docker/compose/releases/download/v2.7.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "sudo chmod +x /usr/local/bin/docker-compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip --quiet\n",
    "    !{sys.executable} -m pip install -U sagemaker transformers huggingface_hub --quiet\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "import huggingface_hub\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "\n",
    "artifacts_bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "execution_role_arn = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/.cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import instance_types\n",
    "\n",
    "ref_model_id = \"meta-textgeneration-llama-3-2-3b\"\n",
    "instance_type = instance_types.retrieve_default(\n",
    "    model_id=ref_model_id,\n",
    "    model_version=\"1.1.2\",\n",
    "    scope=\"inference\")\n",
    "print(instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 1**] 모델 아티팩트 및 서빙/스코어링 로직 저장\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id='meta-llama/Llama-3.2-3B'\n",
    "\n",
    "model_name = model_id.split(\"/\")[-1].lower()\n",
    "model_name = model_name.replace(\".\", \"-\")\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_local_download_dir = Path.cwd() / model_name\n",
    "hf_local_download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=model_id,\n",
    "    revision=\"main\",\n",
    "    local_dir=hf_local_download_dir,\n",
    "    # use_auth_token=\"hf_your_access_token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf shell && mkdir shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile shell/model_compression_upload.sh\n",
    "\n",
    "cd llama-3-2-3b\n",
    "tar cvf - * | pigz > model.tar.gz\n",
    "\n",
    "cd ..\n",
    "sudo rm -rf compressed_model && mkdir compressed_model\n",
    "mv llama-3-2-3b/model.tar.gz compressed_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sh ./shell/model_compression_upload.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 2**] 추론 수행을 위한 SageMaker 컨테이너 생성 및 ECR 등록\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_uri = get_huggingface_llm_image_uri(\n",
    "#   backend=\"huggingface\", # or lmi\n",
    "#   region=region\n",
    "# )\n",
    "# image_uri\n",
    "image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.6.0-tgi3.1.1-gpu-py311-cu124-ubuntu22.04-v2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account = sagemaker.Session().account_id()\n",
    "ecr_image_uri = image_uri.replace(\"763104351884\", account)\n",
    "ecr_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf docker && mkdir docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile docker/sagemaker-entrypoint.sh\n",
    "#!/bin/bash\n",
    "\n",
    "if [[ -z \"${HF_MODEL_ID}\" ]]; then\n",
    "  echo \"HF_MODEL_ID must be set\"\n",
    "  exit 1\n",
    "fi\n",
    "export MODEL_ID=\"${HF_MODEL_ID}\"\n",
    "\n",
    "if [[ -n \"${HF_MODEL_REVISION}\" ]]; then\n",
    "  export REVISION=\"${HF_MODEL_REVISION}\"\n",
    "fi\n",
    "\n",
    "if [[ -n \"${SM_NUM_GPUS}\" ]]; then\n",
    "    NUM_SHARD=\"${SM_NUM_GPUS}\"\n",
    "else\n",
    "    NUM_SHARD=$(nvidia-smi --list-gpus | wc -l)\n",
    "fi\n",
    "\n",
    "export NUM_SHARD\n",
    "\n",
    "\n",
    "if [[ -n \"${HF_MODEL_QUANTIZE}\" ]]; then\n",
    "  export QUANTIZE=\"${HF_MODEL_QUANTIZE}\"\n",
    "fi\n",
    "\n",
    "if [[ -n \"${HF_MODEL_TRUST_REMOTE_CODE}\" ]]; then\n",
    "  export TRUST_REMOTE_CODE=\"${HF_MODEL_TRUST_REMOTE_CODE}\"\n",
    "fi\n",
    "\n",
    "text-generation-launcher --port 8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.6.0-tgi3.1.1-gpu-py311-cu124-ubuntu22.04-v2.0\n",
    "\n",
    "ENV HF_MODEL_ID \"/opt/ml/model\"\n",
    "ENV HF_MODEL_QUANTIZE \"bitsandbytes\"\n",
    "ENV HF_MODEL_TRUST_REMOTE_CODE \"true\"\n",
    "# ENV SM_NUM_GPUS \"4\"\n",
    "RUN pip install --no-cache-dir numpy==1.24.3\n",
    "\n",
    "COPY sagemaker-entrypoint.sh entrypoint.sh\n",
    "RUN chmod +x entrypoint.sh\n",
    "\n",
    "ENTRYPOINT [\"./entrypoint.sh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile docker/build_and_push.sh\n",
    "\n",
    "original_image_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.6.0-tgi3.1.1-gpu-py311-cu124-ubuntu22.04-v2.0\"\n",
    "\n",
    "algorithm_name=\"huggingface-pytorch-tgi-inference\"\n",
    "\n",
    "cd docker\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "target_image_uri=\"${account}.dkr.ecr.us-west-2.amazonaws.com/${algorithm_name}:2.6.0-tgi3.1.1-gpu-py311-cu124-ubuntu22.04-v2.0\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin \"763104351884.dkr.ecr.us-west-2.amazonaws.com\"\n",
    "\n",
    "# docker pull $original_image_uri\n",
    "# docker image tag $original_image_uri $target_image_uri\n",
    "\n",
    "docker build -f Dockerfile -t ${target_image_uri} .\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${target_image_uri}\n",
    "\n",
    "docker push ${target_image_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sh ./docker/build_and_push.sh > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 3**] SageMaker에서 모델을 성공적으로 호스팅할 수 있는 컨테이너 검증\n",
    "---\n",
    "\n",
    "SageMaker 호스팅 엔드포인트로 배포하기 전에 로컬 모드 엔드포인트로 배포할 수 있습니다. 로컬 모드는 현재 개발 중인 환경에서 도커 컨테이너를 실행하여 SageMaker 프로세싱/훈련/추론 작업을 에뮬레이트할 수 있습니다. 추론 작업의 경우는 Amazon ECR의 딥러닝 프레임워크 기반 추론 컨테이너를 로컬로 가져오고(docker pull) 컨테이너를 실행하여(docker run) 모델 서버를 시작합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker Endpoint (Local Mode)\n",
    "\n",
    "로컬 모드는 필수로 수행할 필요는 없지만, 디버깅에 많은 도움이 됩니다. 또한, 로컬 모드 사용 시에는 모델을 S3에 반드시 업로드할 필요 없이 로컬 디렉터리에서도 로드할 수 있습니다. (`container` 변수 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = True\n",
    "# local_mode = False\n",
    "if local_mode:\n",
    "    from sagemaker.local import LocalSession\n",
    "    instance_type = \"local_gpu\"\n",
    "    sm_session = LocalSession()\n",
    "    sm_session.config = {'local': {'local_code': True}}\n",
    "    sm_client = sagemaker.local.LocalSagemakerClient()\n",
    "    smr_client = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "    model_data=f\"file://{Path.cwd()}/{model_name}\"\n",
    "else:\n",
    "    instance_type = \"ml.g5.12xlarge\"\n",
    "    sm_session = sagemaker.Session()\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    model_data = f\"{compressed_model_path}/model.tar.gz\"\n",
    "\n",
    "instance_count = 1\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = f\"{model_name}-{ts}\"\n",
    "endpoint_config_name = f\"{model_name}-endpoint-config-{ts}\"\n",
    "endpoint_name = f\"{model_name}-endpoint-{ts}\"\n",
    "model_data\n",
    "\n",
    "print(f'--- SageMaker Model Name: {sm_model_name}')\n",
    "print(f'--- Endpoint Config Name: {endpoint_config_name}')     \n",
    "print(f'--- Endpoint Name: {endpoint_name}')\n",
    "print(f'--- Model Data: {model_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# env_var = {\n",
    "#     'HF_MODEL_ID': \"/opt/ml/model\",\n",
    "#     'SM_NUM_GPUS':'4',\n",
    "#     'HF_MODEL_QUANTIZE':'bitsandbytes',\n",
    "#     'HF_MODEL_TRUST_REMOTE_CODE' : 'true'\n",
    "# }\n",
    "\n",
    "env_var = {\n",
    "}\n",
    "\n",
    "container = {\n",
    "    \"Image\": ecr_image_uri,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "    \"Environment\": env_var\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, \n",
    "    ExecutionRoleArn=execution_role_arn, \n",
    "    PrimaryContainer=container,\n",
    ")\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            'ModelDataDownloadTimeoutInSeconds': 300,\n",
    "            'ContainerStartupHealthCheckTimeoutInSeconds': 300,\n",
    "            \n",
    "        },\n",
    "    ],\n",
    ")\n",
    "#print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker kill 162db8246dcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_tokens\":256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_tokens\": 512,\n",
    "        \"stop\": [\"<|eot_id|>\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Accept=\"application/json\",\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(sample_input)\n",
    ")\n",
    "data = response[\"Body\"].read()\n",
    "output = json.loads(data)\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    EndpointConfigName = response['EndpointConfigName']\n",
    "    \n",
    "    response = client.describe_endpoint_config(EndpointConfigName=EndpointConfigName)\n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "    \n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint_config(EndpointConfigName=EndpointConfigName) \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "   \n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint_config: {EndpointConfigName}')     \n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 3**] SageMaker에서 모델을 성공적으로 호스팅할 수 있는 컨테이너 검증\n",
    "---\n",
    "\n",
    "SageMaker 호스팅 엔드포인트로 배포하기 전에 로컬 모드 엔드포인트로 배포할 수 있습니다. 로컬 모드는 현재 개발 중인 환경에서 도커 컨테이너를 실행하여 SageMaker 프로세싱/훈련/추론 작업을 에뮬레이트할 수 있습니다. 추론 작업의 경우는 Amazon ECR의 딥러닝 프레임워크 기반 추론 컨테이너를 로컬로 가져오고(docker pull) 컨테이너를 실행하여(docker run) 모델 서버를 시작합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model_path = f\"s3://{artifacts_bucket_name}/{model_name}/compressed_model\"\n",
    "compressed_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync ./compressed_model/ $compressed_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Endpoint (Local Mode)\n",
    "\n",
    "로컬 모드는 필수로 수행할 필요는 없지만, 디버깅에 많은 도움이 됩니다. 또한, 로컬 모드 사용 시에는 모델을 S3에 반드시 업로드할 필요 없이 로컬 디렉터리에서도 로드할 수 있습니다. (`container` 변수 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    from sagemaker.local import LocalSession\n",
    "    instance_type = \"local_gpu\"\n",
    "    sm_session = LocalSession()\n",
    "    sm_session.config = {'local': {'local_code': True}}\n",
    "    sm_client = sagemaker.local.LocalSagemakerClient()\n",
    "    smr_client = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "    model_data=f\"file://{Path.cwd()}/{model_name}\"\n",
    "else:\n",
    "    instance_type = \"ml.g5.12xlarge\" ###### instance type\n",
    "    \n",
    "    sm_session = sagemaker.Session()\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    model_data = f\"{compressed_model_path}/model.tar.gz\"\n",
    "\n",
    "instance_count = 1\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = f\"{model_name}-{ts}\"\n",
    "endpoint_config_name = f\"{model_name}-endpoint-config-{ts}\"\n",
    "endpoint_name = f\"{model_name}-endpoint-{ts}\"\n",
    "\n",
    "print(f'--- SageMaker Model Name: {sm_model_name}')\n",
    "print(f'--- Endpoint Config Name: {endpoint_config_name}')     \n",
    "print(f'--- Endpoint Name: {endpoint_name}')\n",
    "print(f'--- Model Data: {model_data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_var = {}\n",
    "\n",
    "container = {\n",
    "    \"Image\": ecr_image_uri,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "    \"Environment\": env_var\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, \n",
    "    ExecutionRoleArn=execution_role_arn, \n",
    "    PrimaryContainer=container,\n",
    ")\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            'ModelDataDownloadTimeoutInSeconds': 300,\n",
    "            'ContainerStartupHealthCheckTimeoutInSeconds': 300\n",
    "            \n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "def make_console_link(region, endpoint_name, task='[SageMaker LLM Serving]'):\n",
    "    endpoint_link = f'<b> {task} <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">Check Endpoint Status</a></b>'   \n",
    "    return endpoint_link\n",
    "\n",
    "endpoint_link = make_console_link(region, endpoint_name)\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(30)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_tokens\":256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_tokens\": 512,\n",
    "        \"stop\": [\"<|eot_id|>\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Accept=\"application/json\",\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(sample_input)\n",
    ")\n",
    "data = response[\"Body\"].read()\n",
    "output = json.loads(data)\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    EndpointConfigName = response['EndpointConfigName']\n",
    "    \n",
    "    response = client.describe_endpoint_config(EndpointConfigName=EndpointConfigName)\n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "    \n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint_config(EndpointConfigName=EndpointConfigName) \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "   \n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint_config: {EndpointConfigName}')     \n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 4**] ML 모델을 모델 패키지로 패키징\n",
    "---\n",
    "이 **step**에서는 아티팩트(ECR 이미지 및 학습된 모델 아티팩트)를 ModelPackage로 패키징하는 방법을 살펴봅니다. 이 작업을 완료하면 AWS 마켓플레이스에서 제품을 사전 학습된 모델로 등록할 수 있습니다.\n",
    "\n",
    "**Note:** 모델을 여러 하드웨어 유형(CPU/GPU/Inferentia)에 배포할 수 있는 경우, 일반적으로 사용되는 컨테이너 이미지가 각각 다르기 때문에 각각에 대해 모델패키지를 생성하고 MP 목록에 다른 버전으로 추가해야 합니다.  \n",
    "\n",
    "### 모델 패키지 사전 준비\n",
    "모델 패키지는 추론에 필요한 모든 요소를 패키지로 묶은 모델 아티팩트에 대한 재사용 가능한 추상화 형태입니다. 이는 모델 데이터 위치(선택 사항)와 함께 사용할 추론 이미지를 정의하는 추론 사양으로 구성됩니다. ModelPackage는 AWS 마켓플레이스에 판매자로 등록할 AWS 계정에서 생성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "import huggingface_hub\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "\n",
    "artifacts_bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "execution_role_arn = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_client = sagemaker_session.boto_session.client(\"s3\")\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id='meta-llama/Llama-3.2-3B'\n",
    "\n",
    "model_name = model_id.split(\"/\")[-1].lower()\n",
    "model_name = model_name.replace(\".\", \"-\")\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compressed_model_path = f\"s3://{artifacts_bucket_name}/{model_name}/compressed_model\"\n",
    "compressed_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data = f\"{compressed_model_path}/model.tar.gz\"\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"huggingface\", # or lmi\n",
    "  region=region\n",
    ")\n",
    "account = sagemaker.Session().account_id()\n",
    "ecr_image_uri = image_uri.replace(\"763104351884\", account)\n",
    "ecr_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 패키지 생성\n",
    "모델 패키지 생성 프로세스에서는 다음을 지정해야 합니다:\n",
    "  1. 도커 이미지\n",
    "  2. 모델 아티팩트\n",
    "    - tar.gz 형태로 압축된 모델 아티팩트가 제공되어야 합니다.\n",
    "        \n",
    "판매자(및 구매자)에게 Amazon SageMaker에서 제품이 작동한다는 확신을 주기 위해, AWS Marketplace에 제품을 리스팅하기 전에 SageMaker는 기본적인 유효성 검사를 위와 같이 진행하였습니다. 이 유효성 검사 프로세스가 성공해야만 제품을 AWS Marketplace에 리스팅할 수 있습니다. 이 유효성 검사 프로세스는 사용자가 제공한 유효성 검사 프로필과 샘플 데이터를 사용하여 모델을 사용하여 계정에서 변환 작업을 생성하여 추론 이미지가 SageMaker에서 작동하는지 확인합니다.\n",
    "\n",
    "다음으로, ML 모델에 적합한 인스턴스 크기를 식별해야 하며, ML 모델 위에서 성능 테스트를 실행하여 이를 확인할 수 있습니다.\n",
    "\n",
    "**Note:** 모델 튜닝 외에도 인스턴스 유형을 식별할 때 모델의 요구 사항을 고려해야 합니다.  모델이 GPU 리소스를 사용하지 않는 경우 GPU 인스턴스 유형을 포함하지 마세요. 마찬가지로 모델이 GPU 리소스를 사용하지만 단일 GPU만 사용할 수 있는 경우, 여러 개의 GPU가 있는 인스턴스 유형을 포함하지 마세요. 성능상의 이점은 없이 사용자의 인프라 요금만 증가시킬 수 있기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트용 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_tokens\":256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_tokens\": 512,\n",
    "        \"stop\": [\"<|eot_id|>\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_line = json.dumps(sample_input)\n",
    "s3_client.put_object(Bucket=artifacts_bucket_name, Key=f\"{model_name}/validation-input-json/input.jsonl\", Body=json_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_file_name = \"input.jsonl\"\n",
    "validation_input_path = f\"s3://{artifacts_bucket_name}/{model_name}/validation-input-json/\"\n",
    "validation_output_path = f\"s3://{artifacts_bucket_name}/{model_name}/validation-output-jsonl/\"\n",
    "validation_input_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = f\"{model_name}-{ts}\"\n",
    "\n",
    "print(f'--- SageMaker Model Name: {sm_model_name}')\n",
    "\n",
    "# Define parameters\n",
    "model_description = \"marketplace-model-test\" #\"<<YourModelDescription>>\"\n",
    "\n",
    "# <<YourSupportedContentTypes>>\n",
    "supported_content_types = [\"application/json\"] #[\"text/csv\", \"application/json\", \"application/json\", \"application/jsonlines\"]\n",
    "\n",
    "# <<YourSupportedResponseMIMETypes>>\n",
    "supported_response_MIME_types = [ \n",
    "    \"application/json\",\n",
    "]\n",
    "\n",
    "supported_realtime_inference_instance_types = [\"ml.g5.2xlarge\", \"ml.g5.4xlarge\", \"ml.g5.12xlarge\", \"ml.g5.16xlarge\", \"ml.g5.24xlarge\",\"ml.g5.48xlarge\"]\n",
    "supported_batch_transform_instance_types = [\"ml.g5.xlarge\"] #  Don't use batch transform. And, the Batch Transform validation step is not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package = sagemaker_session.sagemaker_client.create_model_package(\n",
    "    ModelPackageName=sm_model_name,\n",
    "    ModelPackageDescription=model_description,\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": ecr_image_uri,\n",
    "                \"ModelDataUrl\": model_data\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedTransformInstanceTypes\": supported_batch_transform_instance_types,\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": supported_realtime_inference_instance_types,\n",
    "        \"SupportedContentTypes\": supported_content_types,\n",
    "        \"SupportedResponseMIMETypes\": supported_response_MIME_types,\n",
    "    },\n",
    "    CertifyForMarketplace=True,  # Make sure to set this to True\n",
    "   ValidationSpecification={\n",
    "        'ValidationRole': execution_role_arn,\n",
    "        'ValidationProfiles': [\n",
    "            {\n",
    "                'ProfileName': \"validation\",\n",
    "                'TransformJobDefinition': {\n",
    "                    'MaxConcurrentTransforms': 1,\n",
    "                    'MaxPayloadInMB': 64,\n",
    "                    'BatchStrategy': 'SingleRecord',\n",
    "                    'TransformInput': {\n",
    "                        'DataSource': {\n",
    "                            'S3DataSource': {\n",
    "                                'S3DataType': 'S3Prefix',\n",
    "                                'S3Uri': f'{validation_input_path}input.jsonl'\n",
    "                            }\n",
    "                        },\n",
    "                        'ContentType': 'application/json',\n",
    "                        'CompressionType': 'None',\n",
    "                        'SplitType': 'None'\n",
    "                    },\n",
    "                    'TransformOutput': {\n",
    "                        'S3OutputPath': f'{validation_output_path}output.json',\n",
    "                        'Accept': 'application/json',\n",
    "                        'AssembleWith': 'None',\n",
    "                    },\n",
    "                    'TransformResources': {\n",
    "                        'InstanceType': supported_batch_transform_instance_types[0],\n",
    "                        'InstanceCount': 1,\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_list = []\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "model_list_pack = sm_client.list_model_packages()\n",
    "model_package_list = model_list_pack['ModelPackageSummaryList']\n",
    "NextToken = model_list_pack.get('NextToken')\n",
    "\n",
    "while True:\n",
    "    if model_list_pack.get('NextToken'):\n",
    "        NextToken = model_list_pack.get('NextToken')\n",
    "        model_list_pack = sm_client.list_model_packages(NextToken=NextToken)\n",
    "        model_package_list.extend(model_list_pack['ModelPackageSummaryList'])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "model_package_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ModelPackageName='meta-llama-3-1-8b-instruct-2024-08-04-23-52-19'\n",
    "ModelPackageName = sm_model_name\n",
    "sm_client.describe_model_package(ModelPackageName=ModelPackageName)['ModelPackageStatusDetails']\n",
    "# sm_client.delete_model_package(ModelPackageName=ModelPackageName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sagemaker_session.wait_for_model_package(model_package_name=sm_model_name) # If failure occurs navigate to SageMaker Console > My marketplace model packages > select the failed ModelPackage for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음을 실행하기 전에, [Model Packages console from Amazon SageMaker](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources)을 열어서 모델 생성의 성공했는지를 확인해야 합니다.\n",
    "모델을 선택하고 **Validation** 탭을 열어서 validation 결과를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 5**] Amazon SageMaker에 배포하여 ML 모델 패키지 검증\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 패키지에서 모델 객체 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker 모델을 Endpoint로 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage\n",
    "\n",
    "model = ModelPackage(\n",
    "    role=execution_role_arn,\n",
    "    model_package_arn=model_package[\"ModelPackageArn\"],\n",
    "    # model_package_arn=\"arn:aws:sagemaker:us-west-2:322537213286:model-package/meta-llama-3-1-8b-instruct-2024-08-04-04-24-21\",\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    # instance_type=supported_realtime_inference_instance_types[0],\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    endpoint_name=sm_model_name,\n",
    "    model_data_download_timeout=600,\n",
    "    container_startup_health_check_timeout=300,\n",
    ")\n",
    "model.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### boto3로 예시 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_tokens\":256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"stop\": [\"<|eot_id|>\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=model.endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(sample_input),\n",
    ")\n",
    "\n",
    "json.load(response[\"Body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 생성된 endpoint configuration 과 endpoint 정리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model.endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 모델은 필수가 아니므로 삭제해도 됩니다. \n",
    "- 배포 가능한 모델을 삭제한다는 점에 유의하세요. \n",
    "- 모델 패키지는 삭제하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AWS 마켓플레이스에 모델을 게시하려면 모델 패키지 ARN을 지정해야 합니다. 다음 모델 패키지 ARN을 복사합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package[\"ModelPackageArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [**Step 6**] AWS Marketplace에 ML 모델 등록\n",
    "---\n",
    "\n",
    "1.  모델 파트너는 AWS 마켓플레이스에서 [public profile](https://docs.aws.amazon.com/marketplace/latest/userguide/seller-registration-process.html#seller-public-profile)을 생성하고 seller로 등록합니다.\n",
    "마켓플레이스의 상품은 무료 상품으로 등록되므로 세금 정보를 제공할 필요가 없습니다.\n",
    "\n",
    "2. 세이지메이커 콘솔의 [Model Packages](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources) 섹션에서 이 노트북에서 생성한 엔티티를 찾을 수 있습니다. 성공적으로 생성되고 유효성이 검사되었다면 해당 엔티티를 선택하고 **Publish new ML Marketplace listing**를 선택할 수 있을 것입니다.\n",
    "\n",
    "<img src=\"images/publish-to-marketplace-action.png\"/>\n",
    "\n",
    "리스팅을 작성할 수 있는 [AWS Marketplace Management portal](https://aws.amazon.com/marketplace/management/ml-products/)로 리디렉션됩니다.\n",
    "\n",
    "<img src=\"images/listing.png\"/>\n",
    "\n",
    "1. 모델이 여러 하드웨어 유형을 대상으로 하는 경우 각 ModelPackage를 별도의 버전으로 목록에 추가하는 것을 잊지 마세요.\n",
    "2. 추가를 클릭하고 모델 정보를 입력합니다. Product visibility을 'Public'로 설정해야 합니다.\n",
    "\n",
    "<img src=\"images/public.png\"/>\n",
    "\n",
    "3. 테스트를 진행할 account 에 대해 모델 접근을 위한 Allowlist에 추가합니다. 예) account `171503325295`, `572320329544` and `559110549532` for access to the model. \n",
    "For region support select: `us-east-1, us-west-2, eu-west-1, eu-central-1, eu-west-2, ap-northeast-1, ap-south-1, ca-central-1, us-east-2, ap-northeast-2`\n",
    "<img src=\"images/allowlist-accs.png\"/>\n",
    "\n",
    "4. Pricing and terms 하에 pricing 모델을 설정합니다.\n",
    "**Inference based pricing (custom metering) at $0**\n",
    "\n",
    "(선택 사항) 컨테이너가 아래를 구현하지 않은 경우 이를 확인하고 다음을 진행하세요. \n",
    "\n",
    "```\n",
    "I confirm that my model package supports the response header for custom metering. Example response header: X-Amzn-Inference-Metering:\n",
    "{\"Dimension\": \"inference.count\", \"ConsumedUnits\": 3}\n",
    "I understand that in absence of this header, default metering will be used instead.\n",
    "```\n",
    "\n",
    "<img src=\"images/inference-based-pricing.png\"/>\n",
    "\n",
    "5. Listing 상태는 다음과 같이 표시되어야 합니다:\n",
    "**Do not click Sign off and publish**\n",
    "\n",
    "<img src=\"images/status-1.png\"/>\n",
    "\n",
    "6. Vissibility status of the listing should be `Limited`.\n",
    "\n",
    "<img src=\"images/status-2.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "* [Publishing your product in AWS Marketplace](https://docs.aws.amazon.com/marketplace/latest/userguide/ml-publishing-your-product-in-aws-marketplace.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "https://medium.com/@aliasghar.arabi/deploy-llama3-on-aws-inferentia-using-sagemaker-lmi-and-djl-serving-aa241db17aa3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
